{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $l2 \\text{-} norm$ vs $l1 \\text{-} norm$\n",
    "### By Jacob Marshall\n",
    "\n",
    "# Introduction\n",
    "Interactively show the difference between the $l2 \\text{-} norm$ and $l1 \\text{-} norm$ in regression. This is done by applying the $l2 \\text{-} norm$ and $l1 \\text{-} norm$ in two cases: Case 1 with an outlier in the sample data and Case 2 with noise in the sample data. The code provided is not critical to understanding the different methods but is needed to generate the interactive widgets.\n",
    "\n",
    "# Nomenclature\n",
    "$x_{i}$ and $y_{i} \\hspace{10mm}$ Actual recorded values of sample data. <br>\n",
    "$\\hat{x}_{i}$ and $\\hat{y}_{i}  \\hspace{10mm}$ Input/Output values from a model, $f(\\hat{x}_{i}) = \\hat{y}_{i}$. It's just good convetion to use the $\\hat{}$ symbol to signify that a value is input/output for a model. <br>\n",
    "$r_{i}  \\hspace{22mm}$ A residual, the difference between an actual recorded value and a predicted value. $r_{i} = y_{i} - \\hat{y}_{i}$ <br>\n",
    "$\\epsilon \\hspace{23mm}$ Model error, calculated by summing all the residuals. The objective of regression is to minimize this error.\n",
    "\n",
    "# Theory\n",
    "## $l2 \\text{-} norm$\n",
    "It is common in regression for the residuals to be squared before summation to find the error. Squaring the residuals makes the error a special kind of error called the summed squared error (SSE). Most people don't realize it but they are using the $l2 \\text{-} norm$.\n",
    "$$\\epsilon = SSE =\\sum^{n-1}_{i=0} r_{i}^2 = \\sum^{n-1}_{i=0} (y_{i} - \\hat{y}_{i})^2$$\n",
    "\n",
    "## $l1 \\text{-} norm$\n",
    "An alternative to the $l2 \\text{-} norm$ is the $l1 \\text{-} norm$ where only the absolute value of the residual is taken before summation to fo find the error. <br>\n",
    "$$ \\epsilon = \\sum^{n-1}_{i=0} \\begin{vmatrix} r_{i} \\end{vmatrix} = \\sum^{n-1}_{i=0} \\begin{vmatrix} y_{i} - \\hat{y}_{i} \\end{vmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code that is common to both cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.optimize import minimize\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values for the two norms\n",
    "l1_norm = 1\n",
    "l2_norm = 2\n",
    "\n",
    "# regresion (class): perform regression on the model y = mx + b\n",
    "class regression():\n",
    "    def __init__(self, y_sample, x_sample, norm):\n",
    "        self.y_sample = y_sample \n",
    "        self.x_sample = x_sample \n",
    "        self.norm = norm\n",
    "        \n",
    "        parameters_guess = (1,1) # initial guess for m and b in model\n",
    "        \n",
    "        self.fit = minimize(self.obj, parameters_guess) # minimize the error (perform regression)\n",
    "        self.parameters = self.fit.x # store regressed parameters\n",
    "        self.error = self.obj(self.parameters)\n",
    "        \n",
    "        # use the regressed parameters in model\n",
    "        self.x_model = np.linspace(min(self.x_sample), max(self.x_sample), 100)\n",
    "        self.y_model = self.model(self.parameters, self.x_model)\n",
    "\n",
    "    def obj(self, parmeters):\n",
    "        y_hat = self.model(parmeters, self.x_sample)\n",
    "        residual = self.y_sample - y_hat\n",
    "        residual_normalized = (np.abs(residual))**self.norm\n",
    "        error = np.sum(residual_normalized)\n",
    "        return error\n",
    "    \n",
    "    def model(self, parameters, x):\n",
    "        # the model is a straight line\n",
    "        m, b = parameters\n",
    "        return m * x + b\n",
    "\n",
    "# plot_helper (def): help graph the results    \n",
    "def plot_helper(y_true, x_true, l1, l2, word):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(l1.x_sample, l1.y_sample, 'gx', label='Sample Data with {}'.format(word)) # plot sample data\n",
    "    plt.plot(x_true, y_true, 'k--', label='True value') # plot true value\n",
    "    plt.plot(l2.x_model, l2.y_model, 'b-', label='Model using l2-norm') # plot l2-norm\n",
    "    plt.plot(l1.x_model, l1.y_model, 'r-', label='Model using l1-norm') # plot l1-norm\n",
    "    plt.grid()\n",
    "    plt.ylim(0,30)\n",
    "    plt.legend(bbox_to_anchor=[1.37, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1: Outlier in sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier(magnitude):\n",
    "    # create true data\n",
    "    n = 20 # number of sample points\n",
    "    y_true = np.ones(n) * 10\n",
    "    x_true = np.linspace(0,100,n)\n",
    "    \n",
    "    # sample data set with an outlier\n",
    "    y_outlier = y_true.copy()\n",
    "    y_outlier[n//3] = y_outlier[n//3] * magnitude # add outlier to y sample data\n",
    "    x_outlier = x_true.copy()\n",
    "    \n",
    "    outlier_l1 = regression(y_outlier, x_outlier, l1_norm)\n",
    "    outlier_l2 = regression(y_outlier, x_outlier, l2_norm)\n",
    "    \n",
    "    plot_helper(y_true, x_true, outlier_l1, outlier_l2, 'outlier')\n",
    "    print('l1 error: {} \\t l1 paramters: {}'.format(outlier_l1.error, outlier_l1.parameters))\n",
    "    print('l2 error: {} \\t l2 paramters: {}'.format(outlier_l2.error, outlier_l2.parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2dbe4bc1b4941d68ee189e21593ca85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.outlier>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "widgets.interact(outlier, magnitude=(1.1,3,0.1)) # adjust the magnitude of the outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model using the $l2 \\text{-} norm$ becomes more biased as the outlier increases. The model using the $l1 \\text{-} norm$ has little change in the event of an outlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2: Noise in sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(sigma):\n",
    "    # create true data\n",
    "    n = 20 # number of sample points\n",
    "    y_true = np.ones(n) * 10\n",
    "    x_true = np.linspace(0,100,n)\n",
    "    \n",
    "    # sample data set with noise\n",
    "    y_noise = y_true.copy()\n",
    "    y_noise = y_noise + np.random.normal(0, sigma, n) # add noise to y sample data\n",
    "    x_noise = x_true.copy()\n",
    "    \n",
    "    noise_l1 = regression(y_noise, x_noise, l1_norm)\n",
    "    noise_l2 = regression(y_noise, x_noise, l2_norm)\n",
    "    \n",
    "    plot_helper(y_true, x_true, noise_l1, noise_l2, 'noise')\n",
    "    print('l1 error: {} \\t l1 paramters: {}'.format(noise_l1.error, noise_l1.parameters))\n",
    "    print('l2 error: {} \\t l2 paramters: {}'.format(noise_l2.error, noise_l2.parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521c1405bc044698a49e2538b654dc91"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.noise>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "widgets.interact(noise, sigma=(0.1,10,0.1)) # adjust the Ïƒ of the noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model using $l2 \\text{-} norm$ tends to perform better than the model using $l1 \\text{-} norm$. However there are times when there isn't much of a difference in performance between the two norms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The disadvantage of the $l2 \\text{-} norm$ is that it makes the model more baised toward outliers in the sample data. The advantage of the $l2 \\text{-} norm$ is that it generally, not always, performs better when there is noise in the sample data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
